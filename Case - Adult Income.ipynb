{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI ML Repository: Adult Income Data Set\n",
    "---\n",
    "### Case Optimum - Paulo Henrique Spada de Moura (Julho/2020)\n",
    "### Aplicação de modelo de Regressão Logística para predição de renda anual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importação das bibliotecas básicas (pandas, numpy, etc). Os pacotes do *sklearn* para aplicação do modelo de Regressão Logística serão importados posteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importação do data set através do repositório da UCI e inserção dos nomes das colunas para identificação dos atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship',\n",
    "                'race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "file = ('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data')\n",
    "adult = pd.read_csv(file,names=colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função abaixo foi criada para apresentar os valores únicos por atributo em nosso data set. Com ela, podemos checar se existem valores \"estranhos\" à coluna e avaliarmos como substituí-los, pois a 'não existência' de valores faltantes em uma base com tantos registros deve despertar alguma desconfiança:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unicos():\n",
    "        for col in colunas:\n",
    "            print(adult[col].unique())\n",
    "check_unicos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a aplicação da função acima, observamos que os valores categóricos do data set apresentam espaçamento à esquerda, no início. Para retirarmos esses espaços, aplicamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Os valores categóricos do dataset estão com um espaçamento à esquerda. Retirando esses espaços:\n",
    "\n",
    "for col in ['workclass','education','marital-status','occupation','relationship','race','sex','native-country','income']:\n",
    "    adult[col] = adult[col].str.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unicos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a correção dos espaçamentos, podemos identificar também a terminologia \"?\" para valores não existentes (por esse motivo não era indicados valores nulos na prévia análise do data set). Nesse passo, serão efetivamente procador por valores nulos (*NaN*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[adult == '?'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conferindo novamente a existência de valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que pouco menos de 5% dos registros apresentam valores únicos. Por representar uma parcela muito pequena do data set, as linhas que apresentam esses valores serão **removidas por completo**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = adult.dropna()\n",
    "adult.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicação da Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto da utilização do algoritmo, trataremos de uma base com parâmetros um pouco mais simples, na qual a aplicação da Regressão Logística, por mais que os dados não sejam exatamente separáveis de maneira linear, será suficiente para realizar as devidas predições e, a posteriori, até mesmo indicar se a aplicação de um modelo mais complexo (por exemplo, Random Forest) retornará uma acuracidade maior ou não. Em suma, será aplicada a Regressão Logística para âmbito de estudo e diversificação dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definição dos dataframes com as **variáveis independentes** e a **variável target (*'income'*)** para construção do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult.drop(['income'],axis=1)\n",
    "y = adult['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Separação das bases de treino e teste, seguindo a recomendação da documentação do data set, que sugere o *split* **70% treino** e **30% teste**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas sklearn para aplicação do modelo\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correção do *encoding* para as variáveis categóricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "categoricas = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "for cat in categoricas:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    X_train[cat] = le.fit_transform(X_train[cat])\n",
    "    X_test[cat] = le.transform(X_test[cat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dimensionamento das variáveis independentes** (*Feature Scaling* - normalização dos intervalos das variáveis independentes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passo 1) Utilização do modelo de regressão logística com **TODAS** as variáveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Acuracidade da regressão logística (todas as variáveis): {} ' .format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passo 2) Aplicação do algoritmo de **análise de componentes principais (PCA)** para utilização das variáveis mais relevantes. Utilizando a função \"*explained_variance_ratio_*\", é indicada a **proporção da variância** para a inserção de cada componente principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na *array* resultante do passo acima,observados a **parcela de variância** pela qual cada variável é responsável no modelo. A seguir, faremos os testes **reduzindo a dimensionalidade**, de acordo com as variâncias de menor significância para avaliação da **acuracidade** do modelo de regressão logística:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passo 2.1) Eliminação da **última variável** (\"*native-country*\" - 2,75% de variância): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult.drop(['income','native-country'], axis=1)\n",
    "y = adult['income']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "categoricas = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
    "for cat in categoricas:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[cat] = le.fit_transform(X_train[cat])\n",
    "        X_test[cat] = le.transform(X_test[cat])\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Acuracidade da regressão logística (13 variáveis): {} '. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passo 2.2) Eliminação das **duas últimas variáveis** (\"*native-country*\" e \"*hours-per-week*\" - ~7% de variância):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult.drop(['income','native-country', 'hours-per-week'], axis=1)\n",
    "y = adult['income']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "categoricas = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
    "for cat in categoricas:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[cat] = le.fit_transform(X_train[cat])\n",
    "        X_test[cat] = le.transform(X_test[cat])\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "print('Acuracidade da regressão logística (12 variáveis): {} '. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passo 2.3) Eliminação das **três últimas variáveis** (\"*native-country*\", \"*hours-per-week*\" e \"*capital-loss*\" - ~12% de variância): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult.drop(['income','native-country', 'hours-per-week', 'capital-loss'], axis=1)\n",
    "y = adult['income']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "categoricas = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
    "for cat in categoricas:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[cat] = le.fit_transform(X_train[cat])\n",
    "        X_test[cat] = le.transform(X_test[cat])\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "print('Acuracidade da regressão logística (11 variáveis): {} '. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando o problema em questão, por contarmos com uma grande quantidade de variáveis independentes (grande número de dimensões para o modelo), escolheremos uma quantidade de dimensões que possam **explicar de maneira significativa uma grande parcela da variância (pelo menos 90%)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult.drop(['income'], axis=1)\n",
    "y = adult['income']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "categoricas = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "for cat in categoricas:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[cat] = le.fit_transform(X_train[cat])\n",
    "        X_test[cat] = le.transform(X_test[cat])\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "\n",
    "pca= PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "dim = np.argmax(cumsum >= 0.90) + 1\n",
    "print('O número de dimensões necessárias para preservar 90% de variância é',dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, analisando a acuracidade dos modelos de regressão logística, usaremos as **12 dimensões mais relevantes**, retornando uma acuracidade de, aproximadamente, **81,33%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A determinação das dimensões com PCA também pode ser auxiliada com recursos gráficos, assim como demonstrado abaixo. Aqui, na curva cumulativa da variância de acordo com a quantidade de dimensões, identificaremos o ponto aproximado de \"cotovelo\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlim(0,14,1)\n",
    "plt.xlabel('Quantidade de dimensões')\n",
    "plt.ylabel('Variância acumulada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelo gráfico, podemos observar que a quantidade de dimensões para representação de **90% de variância** ocorre entre **11~12 dimensões**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
